[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Predicting Stock Index Realised Volatility using Bayesian VARs",
    "section": "",
    "text": "Abstract: This research report explores whether VARs and Bayesian VARs are able to predict realised volatility in equity index markets.\nKeywords. BVARs, Realised Volatility, Stock Indices, SP500"
  },
  {
    "objectID": "index.html#objective-and-motivation",
    "href": "index.html#objective-and-motivation",
    "title": "Predicting Stock Index Realised Volatility using Bayesian VARs",
    "section": "Objective and Motivation",
    "text": "Objective and Motivation\nThis paper seeks to examine the effectiveness of Bayesian VARs as a method for forecasting realised volatility (RV) in equity markets. It will explore whether various Bayesian estimation techniques applied to vector autoregression can accurately predict RV. The accurate prediction of market volatility has many applications in Finance, including the pricing of derivatives and the estimation of risk measures such as Value at Risk."
  },
  {
    "objectID": "index.html#data",
    "href": "index.html#data",
    "title": "Predicting Stock Index Realised Volatility using Bayesian VARs",
    "section": "Data",
    "text": "Data\nThe models will be applied to daily data on realised variances occurring on a group of global stock indices. Stock indices serve as gauges of overall equity market performance and are generally categorised by country. For this analysis we will focus on 10 major global indices, the SPX, DJI, FTSE, GDAXI, FCHI, STOXX50E, N225, AORD, HSI and the STI.\n\n\n\nCode\nIndex Name\nRegion\n\n\n\n\nSPX\nS&P 500\nUS\n\n\nDJI\nDow Jones\nUS\n\n\nFTSE\nFTSE 100\nUK\n\n\nGDAXI\nDAX\nGermany\n\n\nFCHI\nCAC40\nFrance\n\n\nSTOXX50E\nEURO STOXX\nEurope\n\n\nN225\nNIKKEI 225\nJapan\n\n\nAORD\nAll Ordinaries\nAustralia\n\n\nHSI\nHang Seng\nHong Kong\n\n\nSTI\nStraits Times\nSingapore\n\n\n\nRealised variance is a measure of historical volatility occuring in financial time series constructed from intraday high frequency return data.\nRealised variance is defined as the sum of squared returns over specific period:\n\\[\\begin{align}\nRealised\\,Variance = \\sum r^2_t\n\\end{align}\\]\nWhere \\(p_t\\) denotes the price of an asset at time t and \\(r_t\\) is defined as the log return over a predetermined interval, for example 5 minutes:\n\\[\\begin{align}\nr_t = log(p_t / p_{t-1})\n\\end{align}\\]\nRealised volatility RV is then computed as the square root of the realised variance.\n\\[\\begin{align}\nRV = \\sqrt{\\sum r^2_t}\n\\end{align}\\]\nThe RV data is sourced from the Oxford Man Realised Library which provides a number of precalculated volatility metrics, including RV on stock indices spanning multiple years. The data set has been employed widely in the literatur for empirical volatility studies, such as by Dutta and Das (2022) and Brandi and Di Matteo (2022). For this analysis we will utilise the 5 minute RV measure provided in the dataset for each of our 10 indices. The data ranges between January 2000 and June 2018.\n\nFigure 1. Time series plots of original values"
  },
  {
    "objectID": "index.html#model",
    "href": "index.html#model",
    "title": "Predicting Stock Index Realised Volatility using Bayesian VARs",
    "section": "Model",
    "text": "Model\nThe model will follow the standard \\(VAR(p)\\) setup as follows:\n\\[\\begin{align}\nrv_t &= \\mu_0 + A_1 rv_{t-1} + \\dots + A_p rv_{t-p} + \\epsilon_t\\\\\n\\epsilon_t | RV_{t-1} &\\sim iidN_N(0_N, \\Sigma)\n\\end{align}\\]\nWhere \\(rv_t\\) is a vector of log transformed realised variances for our \\(N=10\\) stock indices on day \\(t\\). The \\(A_i\\) matrices are \\(N\\times N\\) matrices of the autoregressive slope parameters.\nThe error term vector \\(\\epsilon_t\\) given the data up to \\(t-1\\) is assumed to be iid multivariate normally distributed of dimension \\(N=10\\), with mean \\(0_N\\) and covariance matrix \\(\\Sigma\\).\nBayesian estimation techniques will be then utilised in conjunction with a number of suitably chosen prior specifications in order to estimate competing models and compute 1 day ahead RV forecasts across all indices. The predictions will be made out of sample and the accuracy of the forecasts will then be assessed according to their root mean squared errors RMSE, given by:\n\\[\\begin{align}\nRMSE = \\sqrt{\\sum(\\hat{rv_i} - rv_i )^2/n}\n\\end{align}\\]\nAssessment of prediction accuracy via the RMSEs will facilitate comparison of various different model specifications, such as the incorporation of different prior distributions and differing assumptions about the parameters of those distributions.\nThe significance of being able to reliably forecast market volatility is primarily seen in the context of financial asset pricing. Volatility of the underlying asset is one of the crucial inputs required in options pricing. With reliable forecasts of stock index volatility one can assess the degree to which options quoted in the market are under or over estimating volatility relative to what is indicated by the historical realised dynamics."
  },
  {
    "objectID": "index.html#estimation-procedure-for-the-baseline-model",
    "href": "index.html#estimation-procedure-for-the-baseline-model",
    "title": "Predicting Stock Index Realised Volatility using Bayesian VARs",
    "section": "Estimation Procedure for the Baseline Model",
    "text": "Estimation Procedure for the Baseline Model\nThe baseline model is as follows:\n\\[\\begin{align}\nY &= XA + U \\\\\nY|X,A,\\Sigma &\\sim MN_{T \\times N} (XA, \\Sigma, I_T)\n\\end{align}\\]\nThis implies the following form for the kernel of the likelihood function:\n\\[\\begin{align}\nL(A,\\Sigma|Y,X) \\propto det(\\Sigma)^{-\\frac{T}{2}}exp(-\\frac{1}{2}tr[\\Sigma^{-1}(Y-XA)'(Y-XA)])\n\\end{align}\\]\nWe assume the usual matrix normal and inverse Wishart natural conjugate priors for \\(A\\) and \\(\\Sigma\\):\n\\[\\begin{align}\np(A,\\Sigma) &= p(A|\\Sigma)p(\\Sigma) \\\\\nA|\\Sigma &\\sim MN_{K \\times N}(\\underline{A},\\Sigma,\\underline{V}) \\\\\n\\Sigma &\\sim IW_N(\\underline{S},\\underline{\\nu})\n\\end{align}\\]\nThe posterior distribution is given by the product of the likelihood and the priors.\n\\[\\begin{align}\np(A,\\Sigma|Y,X) &\\propto \\det(\\Sigma)^{-\\frac{T}{2}} \\\\\n&\\times exp(-\\frac{1}{2}tr[\\Sigma^{-1}(Y-XA)'(Y-XA)]) \\\\\n&\\times \\det(\\Sigma)^{-\\frac{N+K+\\underline{v}+1}{2}} \\\\\n&\\times exp(-\\frac{1}{2}tr[\\Sigma^{-1}(A-\\underline{A})'\\underline(V)^{-1}(A-\\underline{A})]) \\\\\n&\\times exp(-\\frac{1}{2}tr[\\Sigma^{-1}\\underline{S}])\n\\end{align}\\]\nCombining the terms and completing the squares for the terms within the square brackets yields the following posterior distributions for \\(A\\) and \\(\\Sigma\\):\n\\[\\begin{align}\np(A|Y,X,\\Sigma) &= MN_{K \\times N}(\\bar{A}, \\Sigma, \\bar{V}) \\\\\np(\\Sigma|Y,X) &= IW_N(\\bar{S},\\bar{\\nu}) \\\\\n\\\\\n\\bar{V} &= (X'X + \\underline{V}^{-1})^{-1} \\\\\n\\bar{A} &= \\bar{V}(X'Y + \\underline{V}^{-1}\\underline{A}) \\\\\n\\bar{nu} &= T + \\underline{\\nu} \\\\\n\\bar{S} &= \\underline{S} + Y'Y + \\underline{A}'\\underline{V}^{-1}\\underline{A} - \\bar{A}'\\bar{V}^{-1}\\bar{A} \\\\\n\n\\end{align}\\]\nGibbs Sampler code for t-distributed errors:\n\n#Bivariate random walk data\nrw_data = data.frame(matrix(nrow=1000, ncol=2))\nrw_data[,1] = cumsum(rnorm(1000,0,1))\nrw_data[,2] = cumsum(rnorm(1000,0,1))\n\np=1\nN=2\n\nY_ext = (rw_data[(p+1):nrow(rw_data),c(1,2)]) #removing first p observations\nX_ext = matrix(1,nrow(Y_ext),1)               #Creating X matrix\nfor (i in 1:p){\n  X_ext     = cbind(X_ext, (rw_data[(p+1):nrow(rw_data)-i,c(1,2)]))\n}\n\nY_ext = as.matrix(Y_ext)\nX_ext = as.matrix(X_ext)\n\n\n\n#Setting priors and initialising parameters\nA.gprior = matrix(0, 3, 2)\nA_V.gprior = diag(1,3)\nSigma_s.gprior = matrix(1, 2, 2)\nSigma_v.gprior = 3\nlambda_s.gprior = 1\nlambda_v.gprior = 1\n\nlambda.draw = lambda_s.gprior/rchisq(1, lambda_v.gprior)\nlambda.draw = 1\n\nSigma.posterior.draws = array(NA, c(2,2,100))\nA.posterior.draws = array(NA, c(3,2,100))\n\nfor (s in 1:100){\n  \n  lambda.gprior.diag = diag(lambda.draw, nrow(Y_ext))\n  \n  A_V.posterior     = solve(t(X_ext)%*%solve(lambda.gprior.diag)%*%X_ext + solve(A_V.gprior))\n  A.posterior       = A_V.posterior%*%(t(X_ext)%*%solve(lambda.gprior.diag)%*%Y_ext + solve(A_V.gprior)%*%A.gprior)\n  Sigma_s.posterior = t(Y_ext)%*%lambda.gprior.diag%*%Y_ext + t(A.gprior)%*%solve(A_V.gprior)%*%A.gprior + Sigma_s.gprior - t(A.posterior)%*%solve(A_V.posterior)%*%A.posterior\n  Sigma_v.posterior = nrow(Y_ext) + Sigma_v.gprior\n  \n  Sigma.posterior.draws[,,s] = solve(rWishart(1, Sigma_v.posterior, solve(Sigma_s.posterior))[,,1])\n  \n  A.posterior.draws[,,s] = matrix(mvtnorm::rmvnorm(1, mean=as.vector(A.posterior), sigma=Sigma.posterior.draws[,,s]%x%A_V.posterior), ncol=2)\n  \n  lambda_s.posterior = sum(diag(solve(Sigma.posterior.draws[,,s])%*%t(Y_ext - X_ext%*%A.posterior.draws[,,s])%*%(Y_ext - X_ext%*%A.posterior.draws[,,s]))) + lambda_s.gprior\n  lambda_v.posterior = nrow(Y_ext)*2 + lambda_v.gprior\n  \n  lambda.draw = lambda_s.posterior / rchisq(1, lambda_v.posterior)\n  \n}\n\n\napply(Sigma.posterior.draws, 1:2, mean)\napply(A.posterior.draws, 1:2, mean)"
  }
]