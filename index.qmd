---
title: "Predicting Stock Index Realised Volatility using Bayesian VARs"
author: "Nathan Huynh"

format:
  html:
    toc: true
    toc-location: left

execute:
  echo: false

knitr:
  opts_knit:
    root.dir: "C:\\Users\\Nathan\\Documents\\GitHub\\mcxs_report"

bibliography: references.bib
---

> **Abstract:** This research report explores whether VARs and Bayesian VARs are able to predict realised volatility in equity index markets.
>
> **Keywords.** BVARs, Realised Volatility, Stock Indices, SP500

# Research Proposal

## Objective and Motivation

This paper seeks to examine the effectiveness of Bayesian VARs as a method for forecasting realised volatility (**RV**) in equity markets. It will explore whether various Bayesian estimation techniques applied to vector autoregression can accurately predict RV. The accurate prediction of market volatility has many applications in Finance, including the pricing of derivatives and the estimation of risk measures such as Value at Risk.

## Data

The models will be applied to daily data on realised variances occurring on a group of global stock indices. Stock indices serve as gauges of overall equity market performance and are generally categorised by country. For this analysis we will focus on 10 major global indices, the SPX, DJI, FTSE, GDAXI, FCHI, STOXX50E, N225, AORD, HSI and the STI.

| Code     | Index Name    | Region   |
|----------|---------------|----------|
| SPX      | S&P 500       | US       |
| DJI      | Dow Jones     | US       |
| FTSE     | FTSE 100      | UK       |
| GDAXI    | DAX           | Germany  |
| FCHI     | CAC40         | France   |
| STOXX50E | EURO STOXX    | Europe   |
| N225     | NIKKEI 225    | Japan    |
| AORD     | All Ordinaries| Australia|
| HSI      | Hang Seng     | Hong Kong|
| STI      | Straits Times | Singapore|

Realised variance is a measure of historical volatility occuring in financial time series constructed from intraday high frequency return data.

Realised variance is defined as the sum of squared returns over specific period:
```{=tex}
\begin{align}
Realised\,Variance = \sum r^2_t
\end{align}
```
Where $p_t$ denotes the price of an asset at time t and $r_t$ is defined as the log return over a predetermined interval, for example 5 minutes:
```{=tex}
\begin{align}
r_t = log(p_t / p_{t-1})
\end{align}
```
Realised volatility **RV** is then computed as the square root of the realised variance.
```{=tex}
\begin{align}
RV = \sqrt{\sum r^2_t}
\end{align}
```

The RV data is sourced from the [Oxford Man Realised Library](https://oxford-man.ox.ac.uk/) which provides a number of precalculated volatility metrics, including RV on stock indices spanning multiple years. The data set has been employed widely in the literatur for empirical volatility studies, such as by Dutta and Das ([2022](https://onlinelibrary.wiley.com/doi/full/10.1002/fut.22372)) and Brandi and Di Matteo ([2022](https://arxiv.org/pdf/2201.10466.pdf)). For this analysis we will utilise the 5 minute RV measure provided in the dataset for each of our 10 indices. The data ranges between January 2000 and June 2018.

##### Figure 1. Time series plots of original values {style="text-align: center;"}
```{R loading in data}

#Sourcing RV data from the Oxford Man Realized Library

temp <- tempfile()
download.file("https://github.com/onnokleen/mfGARCH/raw/v0.1.9/data-raw/OxfordManRealizedVolatilityIndices.zip",temp)
data <- read.csv(unz(temp, "oxfordmanrealizedvolatilityindices.csv"))
unlink(temp)

#Putting the RV data in a dataframe

SPX = na.omit(xts::xts(data[data$Symbol=='.SPX', 'rv5'],as.Date(data[data$Symbol=='.SPX', 'X'])))
FTSE = na.omit(xts::xts(data[data$Symbol=='.FTSE', 'rv5'],as.Date(data[data$Symbol=='.FTSE', 'X'])))
DAX = na.omit(xts::xts(data[data$Symbol=='.GDAXI', 'rv5'],as.Date(data[data$Symbol=='.GDAXI', 'X'])))
NIKKEI = na.omit(xts::xts(data[data$Symbol=='.N225', 'rv5'],as.Date(data[data$Symbol=='.N225', 'X'])))

df <- as.data.frame(
  merge(SPX, 
        FTSE, 
        DAX, 
        NIKKEI))

colnames(df) <- c("S&P Daily RV","FTSE Daily RV","DAX Daily RV","Nikkei Daily RV")
dates <- as.Date(rownames(df),format = "%Y-%m-%d")
titles <- c("S&P Daily RV","FTSE Daily RV","DAX Daily RV","Nikkei Daily RV")

#Plotting the original values of 4 indices 

par(mfrow=c(2,2), mar=c(3,3,2,2))
for (i in 1:ncol(df)){
  plot(dates, y = df[,i], type = "l", 
       main = titles[i], ylab = "RV", xlab = "Date",
       col = 'blue',
       ylim = c(min(na.omit(df[,i])),max(na.omit(df[,i]))))
}
```
# 

As each of the series follow a similar general pattern only four of the ten indices are plotted for visualisation above. From a visual inspection of the examples above, we can see that realised volatility appears highly non-stationary. There are clear spikes in RV over certain periods in time. These periods of high volatility appear to persist for some time before subsiding, which provides evidence in favour of an autoregressive model specification.

Furthermore, spikes in RV appear to happen around the same times across markets. This provides our main motivation for modelling RV via a VAR specification whereby we can seek to capture the dynamic interrelationships between global equity markets.

For ease of modelling and in order to bring the data closer to normality we will work with the log transformed variable $log(rv_t)$.

##### Figure 2. Time series plots of log transformed values {style="text-align: center;"}
```{r}
#Storing log transformed variables in a dataframe

df.log <- as.data.frame(
  merge(log(SPX), 
        log(FTSE), 
        log(DAX), 
        log(NIKKEI)))
colnames(df.log) <- c("S&P log RV","FTSE log RV","DAX Daily RV","Nikkei log RV")
dates <- as.Date(rownames(df.log),format = "%Y-%m-%d")
new_titles <- c("S&P log RV","FTSE log RV","DAX log RV","Nikkei log RV")

#Plotting the log transformed values of 4 indices

par(mfrow=c(2,2), mar=c(3,3,2,2))
for (i in 1:ncol(df.log)){
  plot(dates, y = df.log[,i], type = "l", ylab = "RV", xlab = "Date",
       main = new_titles[i],
       col = 'blue'
       )
}

```
#

Conducting Augmented Dickey-Fuller tests on the log transformed RVs to test for stationarity results in the rejection of the null hypothesis of non-stationarity for all series at the 0.05 level. This implies stationarity in the log transformed variables.


##### ADF Test Results for Log Transformed 5 Minute RVs {style="text-align: center;"}
```{r, warning=FALSE, echo=FALSE, message=FALSE}

df = as.data.frame(matrix(nrow=0, ncol=2))
colnames(df) = c('Index','P Value')

for (i in c('.SPX', '.DJI', '.FTSE', '.GDAXI','.FCHI','.STOXX50E','.N225','.AORD','.HSI','.STI')){
  testres = tseries::adf.test(log(data[data$Symbol=='.SPX','rv5']))
  p = testres$p.value
  new = c(i, p)
  df[nrow(df)+1, ] = new
}

formattable::formattable(df)

```

## Model

The model will follow the standard $VAR(p)$ setup as follows:

```{=tex}
\begin{align}
rv_t &= \mu_0 + A_1 rv_{t-1} + \dots + A_p rv_{t-p} + \epsilon_t\\
\epsilon_t | RV_{t-1} &\sim iidN_N(0_N, \Sigma)
\end{align}
```

Where $rv_t$ is a vector of log transformed realised variances for our $N=10$ stock indices on day $t$. The $A_i$ matrices are $N\times N$ matrices of the autoregressive slope parameters.

The error term vector $\epsilon_t$ given the data up to $t-1$ is assumed to be iid multivariate normally distributed of dimension $N=10$, with mean $0_N$ and covariance matrix $\Sigma$.

Bayesian estimation techniques will be then utilised in conjunction with a number of suitably chosen prior specifications in order to estimate competing models and compute 1 day ahead RV forecasts across all indices. The predictions will be made out of sample and the accuracy of the forecasts will then be assessed according to their root mean squared errors **RMSE**, given by:

```{=tex}
\begin{align}
RMSE = \sqrt{\sum(\hat{rv_i} - rv_i )^2/n}
\end{align}
```

Assessment of prediction accuracy via the RMSEs will facilitate comparison of various different model specifications, such as the incorporation of different prior distributions and differing assumptions about the parameters of those distributions.

The significance of being able to reliably forecast market volatility is primarily seen in the context of financial asset pricing. Volatility of the underlying asset is one of the crucial inputs required in options pricing. With reliable forecasts of stock index volatility one can assess the degree to which options quoted in the market are under or over estimating volatility relative to what is indicated by the historical realised dynamics.



# Modelling Framework

## Estimation Procedure for the Baseline Model

The baseline model is as follows:

```{=tex}
\begin{align}
Y &= XA + U \\
Y|X,A,\Sigma &\sim MN_{T \times N} (XA, \Sigma, I_T)
\end{align}
```

This implies the following form for the kernel of the likelihood function:

```{=tex}
\begin{align}
L(A,\Sigma|Y,X) \propto det(\Sigma)^{-\frac{T}{2}}exp(-\frac{1}{2}tr[\Sigma^{-1}(Y-XA)'(Y-XA)])
\end{align}
```

We assume the usual matrix normal and inverse Wishart natural conjugate priors for $A$ and $\Sigma$:

```{=tex}
\begin{align}
p(A,\Sigma) &= p(A|\Sigma)p(\Sigma) \\
A|\Sigma &\sim MN_{K \times N}(\underline{A},\Sigma,\underline{V}) \\
\Sigma &\sim IW_N(\underline{S},\underline{\nu})
\end{align}
```

The posterior distribution is given by the product of the likelihood and the priors.

```{=tex}
\begin{align}
p(A,\Sigma|Y,X) &\propto \det(\Sigma)^{-\frac{T}{2}} \\
&\times exp(-\frac{1}{2}tr[\Sigma^{-1}(Y-XA)'(Y-XA)]) \\
&\times \det(\Sigma)^{-\frac{N+K+\underline{v}+1}{2}} \\
&\times exp(-\frac{1}{2}tr[\Sigma^{-1}(A-\underline{A})'\underline(V)^{-1}(A-\underline{A})]) \\
&\times exp(-\frac{1}{2}tr[\Sigma^{-1}\underline{S}])
\end{align}
```


Combining the terms and completing the squares for the terms within the square brackets yields the following posterior distributions for $A$ and $\Sigma$:

```{=tex}
\begin{align}
p(A|Y,X,\Sigma) &= MN_{K \times N}(\bar{A}, \Sigma, \bar{V}) \\
p(\Sigma|Y,X) &= IW_N(\bar{S},\bar{\nu}) \\
\\
\bar{V} &= (X'X + \underline{V}^{-1})^{-1} \\
\bar{A} &= \bar{V}(X'Y + \underline{V}^{-1}\underline{A}) \\
\bar{\nu} &= T + \underline{\nu} \\
\bar{S} &= \underline{S} + Y'Y + \underline{A}'\underline{V}^{-1}\underline{A} - \bar{A}'\bar{V}^{-1}\bar{A} \\

\end{align}
```

Gibbs Sampler code for t-distributed errors:

```{r, echo=TRUE, eval=FALSE}

#Bivariate random walk data
rw_data = data.frame(matrix(nrow=1000, ncol=2))
rw_data[,1] = cumsum(rnorm(1000,0,1))
rw_data[,2] = cumsum(rnorm(1000,0,1))

p=1
N=2

Y_ext = (rw_data[(p+1):nrow(rw_data),c(1,2)]) #removing first p observations
X_ext = matrix(1,nrow(Y_ext),1)               #Creating X matrix
for (i in 1:p){
  X_ext     = cbind(X_ext, (rw_data[(p+1):nrow(rw_data)-i,c(1,2)]))
}

Y_ext = as.matrix(Y_ext)
X_ext = as.matrix(X_ext)



#Setting priors and initialising parameters
A.gprior = matrix(0, 3, 2)
A_V.gprior = diag(1,3)
Sigma_s.gprior = diag(2) # your previous matrix was singular!
Sigma_v.gprior = 3
lambda_s.gprior = 1
lambda_v.gprior = 1

lambda.draw = lambda_s.gprior/rchisq(1, lambda_v.gprior)
lambda.draw = 1

S = 1000 # so much easier to set all the dimensions and loops at once

Sigma.posterior.draws = array(NA, c(2,2,S))
A.posterior.draws = array(NA, c(3,2,S))
lambda.posterior.draws = rep(NA,S)
for (s in 1:S){
  
  lambda.gprior.diag = diag(lambda.draw, nrow(Y_ext))
  
  A_V.posterior     = solve(t(X_ext)%*%diag(1/diag(lambda.gprior.diag))%*%X_ext + solve(A_V.gprior)) #this will be MUCH faster
  A.posterior       = A_V.posterior%*%(t(X_ext)%*%diag(1/diag(lambda.gprior.diag))%*%Y_ext + solve(A_V.gprior)%*%A.gprior)
  Sigma_s.posterior = t(Y_ext)%*%diag(1/diag(lambda.gprior.diag))%*%Y_ext + t(A.gprior)%*%solve(A_V.gprior)%*%A.gprior + Sigma_s.gprior - t(A.posterior)%*%solve(A_V.posterior)%*%A.posterior # you had no inverse!
  Sigma_v.posterior = nrow(Y_ext) + Sigma_v.gprior
  
  Sigma.inv.draw      = rWishart(1, Sigma_v.posterior, solve(Sigma_s.posterior))[,,1]
  Sigma.posterior.draws[,,s] = solve(Sigma.inv.draw)
  
  A.posterior.draws[,,s] = matrix(mvtnorm::rmvnorm(1, mean=as.vector(A.posterior), sigma=Sigma.posterior.draws[,,s]%x%A_V.posterior), ncol=2)
  
  lambda_s.posterior = sum(diag(Sigma.inv.draw%*%t(Y_ext - X_ext%*%A.posterior.draws[,,s])%*%(Y_ext - X_ext%*%A.posterior.draws[,,s]))) + lambda_s.gprior # no need to compute th inverse twice!
  lambda_v.posterior = nrow(Y_ext)*2 + lambda_v.gprior
  
  lambda.draw = lambda_s.posterior / rchisq(1, lambda_v.posterior)
  lambda.posterior.draws[s] = lambda.draw # save this as well!
}

apply(A.posterior.draws, 1:2, mean)
apply(Sigma.posterior.draws, 1:2, mean)
mean(lambda.posterior.draws)
# All seems good!
```
